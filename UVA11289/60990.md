### 题意

给定 $n$ 个红点和 $m$ 个蓝点，求一个形如 $ax+by+cz+d=0$ 的平面使得：

- 所有蓝点都满足 $ax+by+cz+d\leq 0$。

- 所有红点都满足 $ax+by+cz+d>0$。

$\texttt{Data Range:}1\leq n+m\leq 200$

### 题解

神仙题。

这种求一个超平面使得能划分点集的题目可以想到感知机算法。

题目说了至少存在一个解，那么就是说点集样本是线性可分的，可以使用上述算法。

首先先来介绍一般的**感知机模型**。

有 $n$ 个样本，每一个样本有 $m$ 个特征以及一个二元输出类别（这里是 $1$ 和 $-1$），形如这样：

$$(x_1^1,x_2^1,\cdots,x_m^1,y_1),(x_1^2,x_2^2,\cdots,x_m^2,y_2),\cdots,(x_1^n,x_2^n,\cdots,x_m^n,y_n)$$

为了表示方便，我们把 $x_1^i,x_2^i\cdots x_m^i$ 合并成为一个行向量，也就是说

$$\vec{x_i}=(x_i^1,x_i^2,\cdots,x_i^m)$$

所以样本现在可以改写成

$$(\vec{x_1},y_1),(\vec{x_2},y_2),\cdots,(\vec{x_n},y_n)$$

感知机的目标是找到一个超平面 $w_1x_1+w_2x_2+\cdots +w_mx_m+b=0$ 使得对于$y_1=1$的一类点，$w_1x_1+w_2x_2+\cdots +w_mx_m+b>0$，反之亦然。

我们仍然采用向量表示，于是所述平面写成

$$\vec{w}\cdot\vec{x}+b=0$$

但是超平面并不是唯一的，采用不同的初始值会导致结果不同。

所以说，感知机模型大概是如下所示：

对于某一个样本 $\vec{x}$，对该样本的判定结果为

$$y=\operatorname{sgn}(\vec{w}\cdot\vec{x}+b)$$

这个 $\operatorname{sgn}$ 在本题中定义如下：

$$\operatorname{sgn}(x)=\begin{cases}1,&x>0\\0,&x\leq 0\end{cases}$$

接下来，既然感知机算法叫做 $\texttt{Perceptron Learning Algorithm}$，可以看出该模型逐步学习而得到一个解的。正如我们做题一样，需要给出对错，才能依据这个来改进程序最终通过。模型也是一样，我们需要对每一次的判定给出对错。

可以知道，正确分类的样本满足 $y(\operatorname{sgn}(\vec{w}\cdot\vec{x}+b))>0$（这里的 $y$ 是给出的输出类别），而错误分类的样本满足 $y(\operatorname{sgn}(\vec{w}\cdot\vec{x}+b))<0$。我们评估当前的感知机，靠的就是**损失函数**。损失函数的优化目标是使所有被错误分类的样本到超平面的距离之和最小。

一个错误分类的样本 $i$，到当前超平面的距离为

$$\frac{-y_i(\vec{w_i}\cdot\vec{x_i}+b)}{\vert\vec{\omega}\vert}$$

其中 $\vert\vec{\omega}\vert$ 定义为 $\sqrt{\sum\limits_{i=1}^{m}\omega_i^2}$,$\vec{\omega}$ 为法向量，我们取其模长为 $1$，并且设所有错误分类的样本为 $M$，则

$$L(\vec{\omega},b)=-\sum\limits_{\vec{x_i}\in M}y_i(\vec{w_i}\cdot\vec{x_i}+b)$$

感知机模型采用的是随机梯度下降法，所以说我们可以用一个点来更新梯度。损失函数 $L(\vec{\omega},b)$ 的梯度如下:

$$∇_{\vec{w}}L(\vec{\omega},b)=-\sum\limits_{\vec{x_i}\in M}y_i\vec{x_i}$$

$$∇_bL(\vec{\omega},b)=-\sum\limits_{\vec{x_i}\in M}y_i$$

所以我们可以依以下步骤更新：

首先，随机选取一个错误分类样本 $(\vec{x_i},y_i)$，然后考虑这样更新（$\vec{\omega_0}$ 和 $b_0$ 为当前值）：

$$\vec{\omega}\leftarrow \vec{\omega_0}+\eta y_i\vec{x_i}$$

$$b\leftarrow b_0+\eta y_i$$

其中 $\eta$ 是步长，我的解法选取 `0.5`。不断迭代直至符合要求即可。

### 代码

```cpp
#include<bits/stdc++.h>
using namespace std;
typedef int ll;
typedef double db;
const ll MAXN=251;
struct Point3d{
	db x,y,z;
};
Point3d l[MAXN],r[MAXN];
ll cnt,ccnt,error;
db eta=0.5,x,y,z;
db f[4];
inline ll read()
{
    register ll num=0,neg=1;
    register char ch=getchar();
    while(!isdigit(ch)&&ch!='-')
    {
        ch=getchar();
    }
    if(ch=='-')
    {
        neg=-1;
        ch=getchar();
    }
    while(isdigit(ch))
    {
        num=(num<<3)+(num<<1)+(ch-'0');
        ch=getchar();
    }
    return num*neg;
}
inline ll classify(Point3d p)
{
	return f[0]*p.x+f[1]*p.y+f[2]*p.z+f[3]>0;
}
inline ll train(Point3d p,ll type)
{
	ll error=type-classify(p);
	f[0]+=1.0*eta*error*p.x,f[1]+=1.0*eta*error*p.y;
	f[2]+=1.0*eta*error*p.z,f[3]+=1.0*eta*error;
	return error;
}
inline void solve()
{
	for(register int i=0;i<cnt;i++)
	{
		scanf("%lf%lf%lf",&l[i].x,&l[i].y,&l[i].z);
	}
	ccnt=read(),f[0]=f[1]=f[2]=f[3]=0;
	for(register int i=0;i<ccnt;i++)
	{
		scanf("%lf%lf%lf",&r[i].x,&r[i].y,&r[i].z);
	}
	while(1)
	{
		error=0;
		for(register int i=0;i<cnt;i++)
		{
			error|=train(l[i],0);
		}
		for(register int i=0;i<ccnt;i++)
		{
			error|=train(r[i],1);
		}
		if(!error)
		{
			break;
		}
	}
	printf("%.6lf %.6lf %.6lf %.6lf\n",f[0],f[1],f[2],f[3]);
}
int main()
{
	while(scanf("%d",&cnt)==1&&cnt>=0)
	{
		solve();
	}
}
```



