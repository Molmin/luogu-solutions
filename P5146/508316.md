### 思路：
由 $n\le 10^6$ 判断出，要一个时间复杂度为 $O(n \log n)$的做法。

题目要求求 $a_j-a_i$ 的最大值，也就是说，要求找出最大的 $a_j$ 和最小的 $a_i$。

那么可以枚举所有 $a_j$，查询 $1-j$ 中的最小值，寻找最大答案。

当然不能暴力查询最小值，于是采用**线段树**优化到 $\log n$ 的时间。

查询最小值的时间复杂度是 $O(\log n)$，遍历 $a_j$ 的复杂度是 $O(n)$，总复杂度为 $O(n \log n)$。

### 代码：
```
#include <iostream>
#include <cstring>
#include <algorithm>
#include <cstdio>

using namespace std;

typedef long long ll; 

#define int ll

const int N = 1111111;
const int inf = 1e18;

struct SMT {
    int l;
    int r;
    int sum;
    ll mn;
}s[N << 2];

ll a[N];

#define ls(x) ((x) * (2))
#define rs(x) ((x) * (2) + (1))

inline void pushup (int nd) {
    s[nd].sum = s[ls(nd)].sum + s[rs(nd)].sum;
    s[nd].mn = min (s[ls(nd)].mn, s[rs(nd)].mn);
}

inline void build (int nd, int l, int r) {
    s[nd].l = l, s[nd].r = r;

    if (l == r) {
        s[nd].sum = a[l];
        s[nd].mn = a[l];

        return ;
    } 

    int mid = l + r >> 1;

    build (ls(nd), l, mid);
    build (rs(nd), mid + 1, r);

    pushup (nd);
}

ll Query_Min (int nd, int l, int r) {
    if (l <= s[nd].l && r >= s[nd].r) {
        return s[nd].mn;
    }

    int mid = s[nd].l + s[nd].r >>1;

    ll ans = inf;

    if (l <= mid) {
        ans = min (ans, Query_Min (ls(nd), l, r));
    }

    if (r > mid) {
        ans = min (ans, Query_Min (rs(nd), l, r));
    }

    return ans;
}

int n, q;

signed main() {
    scanf ("%lld", &n);

    for (int i = 1; i <= n; i ++) {
        scanf ("%lld", &a[i]);
    }

    build (1, 1, n);

    ll ans = -inf;

    for (int i = 1; i <= n; i ++) {
        ans = max (ans, (a[i] - Query_Min (1, 1, i - 1)));
    }

    printf ("%lld", ans);

    return 0;
} 
```
