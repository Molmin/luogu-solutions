前置芝士：[ST 表](https://oi-wiki.org/ds/sparse-table/)

显然区间 gcd 的值在一定范围内不变，考虑对于所有左端点和 gcd 相同的区间，只取符合条件的最右边的端点。在固定左端点和 gcd 的前提下，右端点可通过二分 + ST 表在 $O(\log^2 n)$ 的时间复杂度内求出。

于是很容易想出一个暴力做法：枚举区间左端点，从左向右依次二分直到 $n$。

事实上，这样做的时间复杂度是正确的，为 $O(Tn \log^3 n)$（虽然看上去不太能通过这道题，但二分总次数和 gcd 的时间复杂度不可能卡满）。

证明：考虑一个区间内的 $a_i$ 和枚举到它之前的 gcd 值 $d$，跨过 $i$ 后存在如下两种情况：

1. $\gcd(d, a_i) = d$

此时显然不会增加时间复杂度。

2. $\gcd(d, a_i) < d$

此时显然会增加 $O(\log^2 n)$ 的时间复杂度。但考虑到 $\gcd(d, a_i) \mid d$，此时 $d$ 至少减少为原来的 $\frac{1}{2}$。由于 $d$ 减到 $1$ 后就不会再减小，减小总次数最多为 $\lfloor \log_2 n \rfloor$ 次。

综上，对于每个区间左端点，时间复杂度为 $O(\log^3 n)$，进而可证得上文所述的总时间复杂度。

代码：
```cpp
#include <stdio.h>
#include <math.h>

typedef long long ll;

ll a[100007], st[100007][17];

ll gcd(ll a, ll b){
	return b == 0 ? a : gcd(b, a % b);
}

inline void init(int n){
	int m = log2(n);
	for (register int i = 1; i <= n; i++){
		st[i][0] = a[i];
	}
	for (register int i = 1; i <= m; i++){
		int id = i - 1, t1 = n - (1 << i) + 1, t2 = 1 << id;
		for (register int j = 1; j <= t1; j++){
			st[j][i] = gcd(st[j][id], st[j + t2][id]);
		}
	}
}

inline ll get_gcd(int l, int r){
	int t = log2(r - l + 1);
	return gcd(st[l][t], st[r - (1 << t) + 1][t]);
}

inline ll max(ll a, ll b){
	return a > b ? a : b;
}

int main(){
	int t;
	scanf("%d", &t);
	for (register int i = 1; i <= t; i++){
		int n;
		ll ans = 0;
		scanf("%d", &n);
		for (register int j = 1; j <= n; j++){
			scanf("%lld", &a[j]);
		}
		init(n);
		for (register int j = 1; j <= n; j++){
			int k = j;
			for (register ll x = a[j]; k <= n; x = get_gcd(j, k)){
				int l = k, r = n, pos = n;
				while (l <= r){
					int mid = (l + r) >> 1;
					if (get_gcd(j, mid) == x){
						l = mid + 1;
						pos = mid;
					} else {
						r = mid - 1;
					}
				}
				ans = max(ans, x * (pos - j + 1));
				k = l + 1;
			}
		}
		printf("%lld\n", ans);
	}
	return 0;
}
```